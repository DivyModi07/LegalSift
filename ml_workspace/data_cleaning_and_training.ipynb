{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b6c9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded successfully.\n",
      "Original shape: 562 rows, 11 columns\n",
      "\n",
      "✅ Rows with any missing data have been removed.\n",
      "   - Rows removed: 32\n",
      "   - Shape after dropping nulls: 530 rows, 11 columns\n",
      "\n",
      "✅ Cleaning the 'full_legal_text' column...\n",
      "   - Text cleaning complete.\n",
      "\n",
      "✅ Cleaned data has been saved to 'IPC_Sections_cleaned.csv'.\n",
      "\n",
      "--- Preview of Cleaned Data ---\n",
      "                                        cleaned_text mapped_category  \\\n",
      "0  this act shall be called the indian penal code...    Introduction   \n",
      "1  every person shall be liable to punishment und...    Introduction   \n",
      "2  any person liable by any indian law to be trie...    Introduction   \n",
      "3  the provisions of this code apply also to any ...    Introduction   \n",
      "4  nothing in this act is intended to repeal vary...    Introduction   \n",
      "\n",
      "  urgency_label  \n",
      "0           Low  \n",
      "1           Low  \n",
      "2           Low  \n",
      "3           Low  \n",
      "4           Low  \n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the input and output file paths.\n",
    "# This makes the script easier to manage.\n",
    "INPUT_CSV_PATH = 'IPC_Sections_Final.csv'\n",
    "OUTPUT_CSV_PATH = 'IPC_Sections_cleaned.csv'\n",
    "\n",
    "# --- Helper Function ---\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing special characters, extra whitespace,\n",
    "    and converting to lowercase. Handles non-string inputs gracefully.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Return an empty string if the input is not a string\n",
    "    \n",
    "    # Replace newline characters with a space\n",
    "    text = re.sub(r'\\\\n', ' ', text)\n",
    "    # Remove any character that is not a letter, number, or whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Replace multiple whitespace characters with a single space and strip leading/trailing spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# 1. Load the dataset\n",
    "if not os.path.exists(INPUT_CSV_PATH):\n",
    "    print(f\"Error: The file '{INPUT_CSV_PATH}' was not found in the ml_workspace.\")\n",
    "    print(\"Please make sure the CSV file is in the same directory as this notebook.\")\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_CSV_PATH)\n",
    "    print(\"✅ Dataset loaded successfully.\")\n",
    "    print(f\"Original shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "    # 2. Handle missing values: Drop any row that has at least one null value\n",
    "    original_rows = len(df)\n",
    "    df.dropna(inplace=True)\n",
    "    new_rows = len(df)\n",
    "    print(f\"\\n✅ Rows with any missing data have been removed.\")\n",
    "    print(f\"   - Rows removed: {original_rows - new_rows}\")\n",
    "    print(f\"   - Shape after dropping nulls: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "    # 3. Clean the 'full_legal_text' column to create a new 'cleaned_text' column\n",
    "    print(\"\\n✅ Cleaning the 'full_legal_text' column...\")\n",
    "    df['cleaned_text'] = df['full_legal_text'].apply(clean_text)\n",
    "    print(\"   - Text cleaning complete.\")\n",
    "\n",
    "    # 4. Save the cleaned data to a new CSV file\n",
    "    df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    print(f\"\\n✅ Cleaned data has been saved to '{OUTPUT_CSV_PATH}'.\")\n",
    "\n",
    "    # 5. Display a preview of the key columns to verify the changes\n",
    "    print(\"\\n--- Preview of Cleaned Data ---\")\n",
    "    print(df[['cleaned_text', 'mapped_category', 'urgency_label']].head())\n",
    "    print(\"-----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13cbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Cleaned Data ---\n",
      "✅ Loaded and filtered data with 526 rows.\n",
      "\n",
      "--- Step 2: Training and Saving Final Classifier Pipelines ---\n",
      "✅ Urgency Classifier Pipeline saved successfully.\n",
      "✅ Category Classifier Pipeline saved successfully.\n",
      "\n",
      "✅✅✅ Mission Accomplished! The final, robust classifiers have been built and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEANED_CSV_PATH = 'IPC_Sections_cleaned.csv'\n",
    "MODEL_SAVE_DIR = '../backend/apps/mlengine/saved_models/'\n",
    "\n",
    "print(\"--- Step 1: Loading Cleaned Data ---\")\n",
    "df = pd.read_csv(CLEANED_CSV_PATH)\n",
    "# Filter out rare categories to ensure data consistency\n",
    "category_counts = df['mapped_category'].value_counts()\n",
    "rare_categories = category_counts[category_counts < 2].index.tolist()\n",
    "if rare_categories:\n",
    "    df = df[~df['mapped_category'].isin(rare_categories)]\n",
    "print(f\"✅ Loaded and filtered data with {len(df)} rows.\")\n",
    "\n",
    "# --- Define features (X) and target variables (y) ---\n",
    "X = df['cleaned_text']\n",
    "y_urgency = df['urgency_label']\n",
    "y_category = df['mapped_category']\n",
    "\n",
    "# --- Split data for training and testing ---\n",
    "X_train, X_test, y_urgency_train, y_urgency_test, y_category_train, y_category_test = train_test_split(\n",
    "    X, y_urgency, y_category, test_size=0.2, random_state=42, stratify=y_category\n",
    ")\n",
    "print(\"\\n--- Step 2: Training and Saving Final Classifier Pipelines ---\")\n",
    "\n",
    "# --- Urgency Classifier Pipeline ---\n",
    "# This bundles the vectorizer and the model together\n",
    "urgency_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "urgency_pipeline.fit(X_train, y_urgency_train)\n",
    "# Save the entire pipeline object\n",
    "joblib.dump(urgency_pipeline, os.path.join(MODEL_SAVE_DIR, 'urgency_classifier.joblib'))\n",
    "print(\"✅ Urgency Classifier Pipeline saved successfully.\")\n",
    "\n",
    "# --- Category Classifier Pipeline ---\n",
    "category_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "category_pipeline.fit(X_train, y_category_train)\n",
    "# Save the entire pipeline object\n",
    "joblib.dump(category_pipeline, os.path.join(MODEL_SAVE_DIR, 'category_classifier.joblib'))\n",
    "print(\"✅ Category Classifier Pipeline saved successfully.\")\n",
    "\n",
    "print(\"\\n✅✅✅ Mission Accomplished! The final, robust classifiers have been built and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ae1600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Enriching Data ---\n",
      "✅ Loaded and filtered data with 526 rows.\n",
      "✅ Data has been enriched with specialist keywords.\n",
      "\n",
      "--- Step 2: Building the Final, Enriched Model ---\n",
      "✅ Using device: CUDA\n",
      "   - Loading model: 'sentence-transformers/all-MiniLM-L6-v2'...\n",
      "   - Generating new embeddings from enriched text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 17/17 [00:00<00:00, 18.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Building final FAISS index...\n",
      "\n",
      "✅✅✅ Mission Accomplished! The final, intelligent model has been built and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEANED_CSV_PATH = 'IPC_Sections_cleaned.csv'\n",
    "MODEL_SAVE_DIR = '../backend/apps/mlengine/saved_models/'\n",
    "DATA_LOOKUP_PATH = os.path.join(MODEL_SAVE_DIR, 'ipc_data_for_index.pkl')\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: ENRICH DATA WITH SPECIALIST KNOWLEDGE\n",
    "# ==============================================================================\n",
    "print(\"--- Step 1: Enriching Data ---\")\n",
    "\n",
    "df = pd.read_csv(CLEANED_CSV_PATH)\n",
    "# Filter out rare categories to ensure data consistency\n",
    "category_counts = df['mapped_category'].value_counts()\n",
    "rare_categories = category_counts[category_counts < 2].index.tolist()\n",
    "if rare_categories:\n",
    "    df = df[~df['mapped_category'].isin(rare_categories)]\n",
    "print(f\"✅ Loaded and filtered data with {len(df)} rows.\")\n",
    "\n",
    "# --- Define specialist terms to bridge the context gap ---\n",
    "theft_keywords = \"theft stole stolen snatching robbery pickpocket\"\n",
    "nuisance_keywords = \"public annoyance disturbance loud music noise party fighting argument\"\n",
    "fraud_keywords = \"fraud cheat scam online bank account money\"\n",
    "harassment_keywords = \"harassment stalking threatening messages bother safety intimidate\"\n",
    "\n",
    "# --- Create the enriched text column ---\n",
    "# Start with the original text\n",
    "df['enriched_text'] = df['cleaned_text']\n",
    "\n",
    "def add_keywords_to_category(df, category, keywords):\n",
    "    \"\"\"Finds all laws in a category and appends keywords to their text.\"\"\"\n",
    "    df.loc[df['mapped_category'] == category, 'enriched_text'] += \" \" + keywords\n",
    "    return df\n",
    "\n",
    "# --- Teach the model by enriching categories ---\n",
    "df = add_keywords_to_category(df, 'Theft', theft_keywords)\n",
    "df = add_keywords_to_category(df, 'Public Nuisance', nuisance_keywords)\n",
    "df = add_keywords_to_category(df, 'Fraud', fraud_keywords)\n",
    "df = add_keywords_to_category(df, 'Criminal Intimidation', harassment_keywords)\n",
    "\n",
    "print(\"✅ Data has been enriched with specialist keywords.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: BUILD THE FINAL SEMANTIC MODEL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Step 2: Building the Final, Enriched Model ---\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"✅ Using device: {device.upper()}\")\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"   - Loading model: '{model_name}'...\")\n",
    "semantic_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "# Generate embeddings from the NEW 'enriched_text' column\n",
    "print(\"   - Generating new embeddings from enriched text...\")\n",
    "corpus_embeddings = semantic_model.encode(df['enriched_text'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "corpus_embeddings_np = corpus_embeddings.cpu().numpy().astype('float32')\n",
    "\n",
    "# Build and save the final FAISS index\n",
    "print(\"   - Building final FAISS index...\")\n",
    "embedding_dimension = corpus_embeddings_np.shape[1]\n",
    "final_faiss_index = faiss.IndexFlatL2(embedding_dimension)\n",
    "final_faiss_index.add(corpus_embeddings_np)\n",
    "\n",
    "# Save the final assets\n",
    "faiss.write_index(final_faiss_index, os.path.join(MODEL_SAVE_DIR, 'faiss_index.index'))\n",
    "df_for_lookup = df[['section_number', 'title', 'short_description']].reset_index(drop=True)\n",
    "df_for_lookup.to_pickle(DATA_LOOKUP_PATH)\n",
    "\n",
    "print(\"\\n✅✅✅ Mission Accomplished! The final, intelligent model has been built and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b0adda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Loading and Enriching Data ---\n",
      "✅ Loaded and filtered data with 526 rows.\n",
      "✅ Data has been enriched with specialist keywords.\n",
      "\n",
      "--- Part 2: Training Final Classifier Pipelines ---\n",
      "✅ Urgency Classifier Pipeline saved.\n",
      "✅ Category Classifier Pipeline saved.\n",
      "\n",
      "--- Part 3: Building Final Recommendation Model ---\n",
      "✅ Using device: CUDA\n",
      "   - Generating new embeddings from enriched text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 17/17 [00:00<00:00, 24.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Building final FAISS index...\n",
      "\n",
      "✅✅✅ DEFINITIVE MODELS BUILT AND SAVED SUCCESSFULLY! ✅✅✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEANED_CSV_PATH = 'IPC_Sections_cleaned.csv'\n",
    "MODEL_SAVE_DIR = '../backend/apps/mlengine/saved_models/'\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: LOAD AND ENRICH THE DATASET\n",
    "# ==============================================================================\n",
    "print(\"--- Part 1: Loading and Enriching Data ---\")\n",
    "df = pd.read_csv(CLEANED_CSV_PATH)\n",
    "# Filter out rare categories for stability\n",
    "category_counts = df['mapped_category'].value_counts()\n",
    "rare_categories = category_counts[category_counts < 2].index.tolist()\n",
    "if rare_categories:\n",
    "    df = df[~df['mapped_category'].isin(rare_categories)]\n",
    "print(f\"✅ Loaded and filtered data with {len(df)} rows.\")\n",
    "\n",
    "# --- Define specialist terms to bridge the context gap ---\n",
    "theft_keywords = \"theft stole stolen snatching robbery pickpocket chain\"\n",
    "nuisance_keywords = \"public annoyance disturbance loud music noise party fighting argument\"\n",
    "fraud_keywords = \"fraud cheat scam online bank account money\"\n",
    "harassment_keywords = \"harassment stalking threatening messages bother safety intimidate\"\n",
    "\n",
    "# --- Create the enriched text column for the recommendation model ---\n",
    "df['enriched_text'] = df['cleaned_text']\n",
    "def add_keywords_to_category(df, category, keywords):\n",
    "    df.loc[df['mapped_category'] == category, 'enriched_text'] += \" \" + keywords\n",
    "    return df\n",
    "\n",
    "df = add_keywords_to_category(df, 'Theft', theft_keywords)\n",
    "df = add_keywords_to_category(df, 'Public Nuisance', nuisance_keywords)\n",
    "df = add_keywords_to_category(df, 'Fraud', fraud_keywords)\n",
    "df = add_keywords_to_category(df, 'Criminal Intimidation', harassment_keywords)\n",
    "print(\"✅ Data has been enriched with specialist keywords.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TRAIN AND SAVE BULLETPROOF CLASSIFIER PIPELINES\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Part 2: Training Final Classifier Pipelines ---\")\n",
    "X = df['cleaned_text']\n",
    "y_urgency = df['urgency_label']\n",
    "y_category = df['mapped_category']\n",
    "\n",
    "X_train, X_test, y_urgency_train, y_urgency_test, y_category_train, y_category_test = train_test_split(\n",
    "    X, y_urgency, y_category, test_size=0.2, random_state=42, stratify=y_category\n",
    ")\n",
    "\n",
    "# --- Urgency Classifier Pipeline ---\n",
    "urgency_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "urgency_pipeline.fit(X_train, y_urgency_train)\n",
    "joblib.dump(urgency_pipeline, os.path.join(MODEL_SAVE_DIR, 'urgency_classifier.joblib'))\n",
    "print(\"✅ Urgency Classifier Pipeline saved.\")\n",
    "\n",
    "# --- Category Classifier Pipeline ---\n",
    "category_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "category_pipeline.fit(X_train, y_category_train)\n",
    "joblib.dump(category_pipeline, os.path.join(MODEL_SAVE_DIR, 'category_classifier.joblib'))\n",
    "print(\"✅ Category Classifier Pipeline saved.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: BUILD THE FINAL RECOMMENDATION MODEL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Part 3: Building Final Recommendation Model ---\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"✅ Using device: {device.upper()}\")\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "semantic_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "print(\"   - Generating new embeddings from enriched text...\")\n",
    "corpus_embeddings = semantic_model.encode(df['enriched_text'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "corpus_embeddings_np = corpus_embeddings.cpu().numpy().astype('float32')\n",
    "\n",
    "print(\"   - Building final FAISS index...\")\n",
    "final_faiss_index = faiss.IndexFlatL2(corpus_embeddings_np.shape[1])\n",
    "final_faiss_index.add(corpus_embeddings_np)\n",
    "\n",
    "faiss.write_index(final_faiss_index, os.path.join(MODEL_SAVE_DIR, 'faiss_index.index'))\n",
    "df_for_lookup = df[['section_number', 'title', 'short_description']].reset_index(drop=True)\n",
    "df_for_lookup.to_pickle(DATA_LOOKUP_PATH)\n",
    "\n",
    "print(\"\\n✅✅✅ DEFINITIVE MODELS BUILT AND SAVED SUCCESSFULLY! ✅✅✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76aa0ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaning complete! Saved as IPC_Sections_Explore.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"IPC_Sections_Final.csv\")\n",
    "\n",
    "# 1. Drop rows with any empty/null values\n",
    "df = df.dropna()\n",
    "\n",
    "# 2. Drop unwanted columns\n",
    "columns_to_remove = [\"urgency_label\", \"keywords\"]\n",
    "df = df.drop(columns=columns_to_remove, errors=\"ignore\")\n",
    "\n",
    "# 3. Clean special characters from 'full_legal_text' column\n",
    "def clean_text(text):\n",
    "    # Keep only letters, numbers, spaces, and .,!? basic punctuation\n",
    "    return re.sub(r\"[^a-zA-Z0-9\\s.,!?]\", \"\", str(text))\n",
    "\n",
    "if \"full_legal_text\" in df.columns:\n",
    "    df[\"full_legal_text\"] = df[\"full_legal_text\"].apply(clean_text)\n",
    "\n",
    "# 4. Save the cleaned CSV\n",
    "df.to_csv(\"IPC_Sections_Explore.csv\", index=False)\n",
    "\n",
    "print(\"✅ Cleaning complete! Saved as IPC_Sections_Explore.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea27418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Loading and Cleaning Data ---\n",
      "✅ Loaded and cleaned data with 558 rows.\n",
      "\n",
      "--- Part 2: Training and Saving Classifier Pipelines ---\n",
      "✅ Data split into training and testing sets.\n",
      "\n",
      "--- Training Urgency Classifier ---\n",
      "✅ Urgency Classifier Pipeline saved successfully.\n",
      "\n",
      "Urgency Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.76      0.72      0.74        18\n",
      "         Low       0.93      0.89      0.91        83\n",
      "      Medium       0.60      0.82      0.69        11\n",
      "\n",
      "    accuracy                           0.86       112\n",
      "   macro avg       0.76      0.81      0.78       112\n",
      "weighted avg       0.87      0.86      0.86       112\n",
      "\n",
      "\n",
      "--- Training Category Classifier ---\n",
      "✅ Category Classifier Pipeline saved successfully.\n",
      "\n",
      "Category Model Performance:\n",
      "                                                      precision    recall  f1-score   support\n",
      "\n",
      "                                            Abetment       0.75      1.00      0.86         3\n",
      "Contempts of the Lawful Authority of Public Servants       1.00      1.00      1.00         4\n",
      "                                 Crime Against Child       0.50      1.00      0.67         1\n",
      "                                Crime Against Person       1.00      0.56      0.71         9\n",
      "                                 Crime Against Woman       0.40      0.67      0.50         3\n",
      "                               Criminal Intimidation       0.00      0.00      0.00         1\n",
      "                                          Defamation       1.00      1.00      1.00         1\n",
      "  False Evidence and Offences Against Public Justice       0.89      0.89      0.89         9\n",
      "                                               Fraud       0.88      1.00      0.93         7\n",
      "                                  General Exceptions       1.00      1.00      1.00         4\n",
      "                                General Explanations       0.73      0.80      0.76        10\n",
      "                                   Human Trafficking       0.00      0.00      0.00         1\n",
      "                                        Introduction       0.50      1.00      0.67         1\n",
      "                                           Obscenity       1.00      1.00      1.00         1\n",
      "                                      Of Punishments       0.83      1.00      0.91         5\n",
      "                             Offences Affecting Life       0.67      0.67      0.67         3\n",
      "                          Offences Against the State       1.00      1.00      1.00         2\n",
      "                      Offences Relating to Elections       1.00      1.00      1.00         2\n",
      "                       Offences Relating to Religion       0.33      1.00      0.50         1\n",
      "                           Offences against Marriage       0.00      0.00      0.00         1\n",
      "                             Offences against Person       1.00      1.00      1.00         1\n",
      "            Offences against the Public Tranquillity       1.00      1.00      1.00         4\n",
      "          Offences by or relating to Public Servants       1.00      1.00      1.00         3\n",
      "     Offences relating to Coin and Government Stamps       1.00      1.00      1.00         7\n",
      "           Offences relating to Weights and Measures       1.00      1.00      1.00         1\n",
      "   Offences relating to the Army, Navy and Air Force       1.00      1.00      1.00         2\n",
      "                                      Property Crime       0.91      0.62      0.74        16\n",
      "                            Public Health and Safety       1.00      1.00      1.00         2\n",
      "                                     Public Nuisance       0.60      1.00      0.75         3\n",
      "                                       Public Safety       1.00      1.00      1.00         2\n",
      "                            Right of Private Defence       1.00      1.00      1.00         2\n",
      "\n",
      "                                            accuracy                           0.84       112\n",
      "                                           macro avg       0.77      0.85      0.79       112\n",
      "                                        weighted avg       0.85      0.84      0.83       112\n",
      "\n",
      "\n",
      "✅✅✅ DEFINITIVE CLASSIFIERS BUILT AND SAVED SUCCESSFULLY! ✅✅✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DIVY MODI\\LegalSift\\backend\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\DIVY MODI\\LegalSift\\backend\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\DIVY MODI\\LegalSift\\backend\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# This assumes your notebook is in the 'ml_workspace' directory\n",
    "CSV_PATH = 'IPC_Sections_Final.csv'\n",
    "MODEL_SAVE_DIR = '../backend/apps/mlengine/saved_models/'\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: LOAD AND CLEAN THE DATASET\n",
    "# ==============================================================================\n",
    "print(\"--- Part 1: Loading and Cleaning Data ---\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# --- Define a robust text cleaning function ---\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.replace('\\\\n', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "# Apply cleaning and drop rows with missing essential data\n",
    "df['cleaned_text'] = df['full_legal_text'].apply(clean_text)\n",
    "df.dropna(subset=['cleaned_text', 'mapped_category', 'urgency_label'], inplace=True)\n",
    "\n",
    "# Filter out rare categories for model stability\n",
    "category_counts = df['mapped_category'].value_counts()\n",
    "rare_categories = category_counts[category_counts < 2].index.tolist()\n",
    "if rare_categories:\n",
    "    df = df[~df['mapped_category'].isin(rare_categories)]\n",
    "print(f\"✅ Loaded and cleaned data with {len(df)} rows.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TRAIN AND SAVE BULLETPROOF CLASSIFIER PIPELINES\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Part 2: Training and Saving Classifier Pipelines ---\")\n",
    "X = df['cleaned_text']\n",
    "y_urgency = df['urgency_label']\n",
    "y_category = df['mapped_category']\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_urgency_train, y_urgency_test, y_category_train, y_category_test = train_test_split(\n",
    "    X, y_urgency, y_category, test_size=0.2, random_state=42, stratify=y_category\n",
    ")\n",
    "print(\"✅ Data split into training and testing sets.\")\n",
    "\n",
    "# --- Urgency Classifier Pipeline ---\n",
    "print(\"\\n--- Training Urgency Classifier ---\")\n",
    "# This pipeline bundles the vectorizer and the model together\n",
    "urgency_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "urgency_pipeline.fit(X_train, y_urgency_train)\n",
    "# Save the entire pipeline object\n",
    "joblib.dump(urgency_pipeline, os.path.join(MODEL_SAVE_DIR, 'urgency_classifier.joblib'))\n",
    "print(\"✅ Urgency Classifier Pipeline saved successfully.\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_urgency_pred = urgency_pipeline.predict(X_test)\n",
    "print(\"\\nUrgency Model Performance:\")\n",
    "print(classification_report(y_urgency_test, y_urgency_pred))\n",
    "\n",
    "\n",
    "# --- Category Classifier Pipeline ---\n",
    "print(\"\\n--- Training Category Classifier ---\")\n",
    "category_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "category_pipeline.fit(X_train, y_category_train)\n",
    "# Save the entire pipeline object\n",
    "joblib.dump(category_pipeline, os.path.join(MODEL_SAVE_DIR, 'category_classifier.joblib'))\n",
    "print(\"✅ Category Classifier Pipeline saved successfully.\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_category_pred = category_pipeline.predict(X_test)\n",
    "print(\"\\nCategory Model Performance:\")\n",
    "print(classification_report(y_category_test, y_category_pred))\n",
    "\n",
    "\n",
    "print(\"\\n✅✅✅ DEFINITIVE CLASSIFIERS BUILT AND SAVED SUCCESSFULLY! ✅✅✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830c421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Loading and Cleaning Data ---\n",
      "✅ Loaded and cleaned data with 558 rows.\n",
      "\n",
      "--- Part 2: Training Definitive Classifier Pipelines ---\n",
      "✅ Data split into training and testing sets.\n",
      "\n",
      "--- Training Urgency Classifier ---\n",
      "✅ Urgency Classifier Pipeline saved.\n",
      "\n",
      "--- Training Category Classifier ---\n",
      "✅ Category Classifier Pipeline saved.\n",
      "\n",
      "--- Part 3: Verifying the Trained Classifiers ---\n",
      "\n",
      "--- Prediction Results ---\n",
      "\n",
      "Complaint #1: 'While I was walking in the market, someone on a motorcycle snatched my...'\n",
      "   - Predicted Urgency:  Low\n",
      "   - Predicted Category: General Explanations\n",
      "\n",
      "Complaint #2: 'I was tricked into installing a fake payment app and they withdrew 50,...'\n",
      "   - Predicted Urgency:  Low\n",
      "   - Predicted Category: General Explanations\n",
      "\n",
      "Complaint #3: 'A person from my old office has been sending me threatening messages o...'\n",
      "   - Predicted Urgency:  Low\n",
      "   - Predicted Category: General Explanations\n",
      "\n",
      "Complaint #4: 'Someone broke the side mirror of my car while it was parked overnight....'\n",
      "   - Predicted Urgency:  Low\n",
      "   - Predicted Category: General Explanations\n",
      "\n",
      "Complaint #5: 'My neighbors are having a loud party with music blaring at 2 AM and it...'\n",
      "   - Predicted Urgency:  Low\n",
      "   - Predicted Category: General Explanations\n",
      "\n",
      "--------------------------\n",
      "\n",
      "✅✅✅ DEFINITIVE CLASSIFIERS BUILT, SAVED, AND VERIFIED! ✅✅✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "CSV_PATH = 'IPC_Sections_Final.csv'\n",
    "MODEL_SAVE_DIR = '../backend/apps/mlengine/saved_models/'\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: LOAD AND CLEAN THE DATASET\n",
    "# ==============================================================================\n",
    "print(\"--- Part 1: Loading and Cleaning Data ---\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# --- Define the single, authoritative text cleaning function ---\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.replace('\\\\n', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "# Apply cleaning and prepare the DataFrame\n",
    "df['cleaned_text'] = df['full_legal_text'].apply(clean_text)\n",
    "df.dropna(subset=['cleaned_text', 'mapped_category', 'urgency_label'], inplace=True)\n",
    "\n",
    "# Filter out rare categories for model stability\n",
    "category_counts = df['mapped_category'].value_counts()\n",
    "rare_categories = category_counts[category_counts < 2].index.tolist()\n",
    "if rare_categories:\n",
    "    df = df[~df['mapped_category'].isin(rare_categories)]\n",
    "print(f\"✅ Loaded and cleaned data with {len(df)} rows.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TRAIN AND SAVE THE DEFINITIVE CLASSIFIER PIPELINES\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Part 2: Training Definitive Classifier Pipelines ---\")\n",
    "X = df['cleaned_text']\n",
    "y_urgency = df['urgency_label']\n",
    "y_category = df['mapped_category']\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_urgency_train, y_urgency_test, y_category_train, y_category_test = train_test_split(\n",
    "    X, y_urgency, y_category, test_size=0.2, random_state=42, stratify=y_category\n",
    ")\n",
    "print(\"✅ Data split into training and testing sets.\")\n",
    "\n",
    "# --- Urgency Classifier Pipeline ---\n",
    "print(\"\\n--- Training Urgency Classifier ---\")\n",
    "urgency_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "urgency_pipeline.fit(X_train, y_urgency_train)\n",
    "joblib.dump(urgency_pipeline, os.path.join(MODEL_SAVE_DIR, 'urgency_classifier.joblib'))\n",
    "print(\"✅ Urgency Classifier Pipeline saved.\")\n",
    "\n",
    "# --- Category Classifier Pipeline ---\n",
    "print(\"\\n--- Training Category Classifier ---\")\n",
    "category_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "category_pipeline.fit(X_train, y_category_train)\n",
    "joblib.dump(category_pipeline, os.path.join(MODEL_SAVE_DIR, 'category_classifier.joblib'))\n",
    "print(\"✅ Category Classifier Pipeline saved.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: VERIFY THE MODELS WITH A REAL-WORLD TEST\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Part 3: Verifying the Trained Classifiers ---\")\n",
    "\n",
    "test_complaints = [\n",
    "    \"While I was walking in the market, someone on a motorcycle snatched my gold chain and sped away.\",\n",
    "    \"I was tricked into installing a fake payment app and they withdrew 50,000 rupees from my account.\",\n",
    "    \"A person from my old office has been sending me threatening messages on social media and has shown up outside my house.\",\n",
    "    \"Someone broke the side mirror of my car while it was parked overnight.\",\n",
    "    \"My neighbors are having a loud party with music blaring at 2 AM and it's a major disturbance.\"\n",
    "]\n",
    "\n",
    "# **NOTE**: We now pass the RAW, UNCLEANED text directly to the pipelines.\n",
    "# The pipelines will handle the cleaning automatically, simulating the real-world scenario.\n",
    "print(\"\\n--- Prediction Results ---\")\n",
    "for i, complaint in enumerate(test_complaints):\n",
    "    predicted_urgency = urgency_pipeline.predict([complaint])[0]\n",
    "    predicted_category = category_pipeline.predict([complaint])[0]\n",
    "    \n",
    "    print(f\"\\nComplaint #{i+1}: '{complaint[:70]}...'\")\n",
    "    print(f\"   - Predicted Urgency:  {predicted_urgency}\")\n",
    "    print(f\"   - Predicted Category: {predicted_category}\")\n",
    "print(\"\\n--------------------------\")\n",
    "\n",
    "print(\"\\n✅✅✅ DEFINITIVE CLASSIFIERS BUILT, SAVED, AND VERIFIED! ✅✅✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c739a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
