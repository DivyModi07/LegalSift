{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b6c9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded successfully.\n",
      "Original shape: 562 rows, 11 columns\n",
      "\n",
      "✅ Rows with any missing data have been removed.\n",
      "   - Rows removed: 32\n",
      "   - Shape after dropping nulls: 530 rows, 11 columns\n",
      "\n",
      "✅ Cleaning the 'full_legal_text' column...\n",
      "   - Text cleaning complete.\n",
      "\n",
      "✅ Cleaned data has been saved to 'IPC_Sections_cleaned.csv'.\n",
      "\n",
      "--- Preview of Cleaned Data ---\n",
      "                                        cleaned_text mapped_category  \\\n",
      "0  this act shall be called the indian penal code...    Introduction   \n",
      "1  every person shall be liable to punishment und...    Introduction   \n",
      "2  any person liable by any indian law to be trie...    Introduction   \n",
      "3  the provisions of this code apply also to any ...    Introduction   \n",
      "4  nothing in this act is intended to repeal vary...    Introduction   \n",
      "\n",
      "  urgency_label  \n",
      "0           Low  \n",
      "1           Low  \n",
      "2           Low  \n",
      "3           Low  \n",
      "4           Low  \n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the input and output file paths.\n",
    "# This makes the script easier to manage.\n",
    "INPUT_CSV_PATH = 'IPC_Sections_Final.csv'\n",
    "OUTPUT_CSV_PATH = 'IPC_Sections_cleaned.csv'\n",
    "\n",
    "# --- Helper Function ---\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing special characters, extra whitespace,\n",
    "    and converting to lowercase. Handles non-string inputs gracefully.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Return an empty string if the input is not a string\n",
    "    \n",
    "    # Replace newline characters with a space\n",
    "    text = re.sub(r'\\\\n', ' ', text)\n",
    "    # Remove any character that is not a letter, number, or whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Replace multiple whitespace characters with a single space and strip leading/trailing spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# 1. Load the dataset\n",
    "if not os.path.exists(INPUT_CSV_PATH):\n",
    "    print(f\"Error: The file '{INPUT_CSV_PATH}' was not found in the ml_workspace.\")\n",
    "    print(\"Please make sure the CSV file is in the same directory as this notebook.\")\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_CSV_PATH)\n",
    "    print(\"✅ Dataset loaded successfully.\")\n",
    "    print(f\"Original shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "    # 2. Handle missing values: Drop any row that has at least one null value\n",
    "    original_rows = len(df)\n",
    "    df.dropna(inplace=True)\n",
    "    new_rows = len(df)\n",
    "    print(f\"\\n✅ Rows with any missing data have been removed.\")\n",
    "    print(f\"   - Rows removed: {original_rows - new_rows}\")\n",
    "    print(f\"   - Shape after dropping nulls: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "    # 3. Clean the 'full_legal_text' column to create a new 'cleaned_text' column\n",
    "    print(\"\\n✅ Cleaning the 'full_legal_text' column...\")\n",
    "    df['cleaned_text'] = df['full_legal_text'].apply(clean_text)\n",
    "    print(\"   - Text cleaning complete.\")\n",
    "\n",
    "    # 4. Save the cleaned data to a new CSV file\n",
    "    df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    print(f\"\\n✅ Cleaned data has been saved to '{OUTPUT_CSV_PATH}'.\")\n",
    "\n",
    "    # 5. Display a preview of the key columns to verify the changes\n",
    "    print(\"\\n--- Preview of Cleaned Data ---\")\n",
    "    print(df[['cleaned_text', 'mapped_category', 'urgency_label']].head())\n",
    "    print(\"-----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13cbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Cleaned Data ---\n",
      "✅ Loaded and filtered data with 526 rows.\n",
      "\n",
      "--- Step 2: Training and Saving Final Classifier Pipelines ---\n",
      "✅ Urgency Classifier Pipeline saved successfully.\n",
      "✅ Category Classifier Pipeline saved successfully.\n",
      "\n",
      "✅✅✅ Mission Accomplished! The final, robust classifiers have been built and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEANED_CSV_PATH = 'IPC_Sections_cleaned.csv'\n",
    "MODEL_SAVE_DIR = '../backend/apps/mlengine/saved_models/'\n",
    "\n",
    "print(\"--- Step 1: Loading Cleaned Data ---\")\n",
    "df = pd.read_csv(CLEANED_CSV_PATH)\n",
    "# Filter out rare categories to ensure data consistency\n",
    "category_counts = df['mapped_category'].value_counts()\n",
    "rare_categories = category_counts[category_counts < 2].index.tolist()\n",
    "if rare_categories:\n",
    "    df = df[~df['mapped_category'].isin(rare_categories)]\n",
    "print(f\"✅ Loaded and filtered data with {len(df)} rows.\")\n",
    "\n",
    "# --- Define features (X) and target variables (y) ---\n",
    "X = df['cleaned_text']\n",
    "y_urgency = df['urgency_label']\n",
    "y_category = df['mapped_category']\n",
    "\n",
    "# --- Split data for training and testing ---\n",
    "X_train, X_test, y_urgency_train, y_urgency_test, y_category_train, y_category_test = train_test_split(\n",
    "    X, y_urgency, y_category, test_size=0.2, random_state=42, stratify=y_category\n",
    ")\n",
    "print(\"\\n--- Step 2: Training and Saving Final Classifier Pipelines ---\")\n",
    "\n",
    "# --- Urgency Classifier Pipeline ---\n",
    "# This bundles the vectorizer and the model together\n",
    "urgency_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "urgency_pipeline.fit(X_train, y_urgency_train)\n",
    "# Save the entire pipeline object\n",
    "joblib.dump(urgency_pipeline, os.path.join(MODEL_SAVE_DIR, 'urgency_classifier.joblib'))\n",
    "print(\"✅ Urgency Classifier Pipeline saved successfully.\")\n",
    "\n",
    "# --- Category Classifier Pipeline ---\n",
    "category_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "category_pipeline.fit(X_train, y_category_train)\n",
    "# Save the entire pipeline object\n",
    "joblib.dump(category_pipeline, os.path.join(MODEL_SAVE_DIR, 'category_classifier.joblib'))\n",
    "print(\"✅ Category Classifier Pipeline saved successfully.\")\n",
    "\n",
    "print(\"\\n✅✅✅ Mission Accomplished! The final, robust classifiers have been built and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ae1600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Enriching Data ---\n",
      "✅ Loaded and filtered data with 526 rows.\n",
      "✅ Data has been enriched with specialist keywords.\n",
      "\n",
      "--- Step 2: Building the Final, Enriched Model ---\n",
      "✅ Using device: CUDA\n",
      "   - Loading model: 'sentence-transformers/all-MiniLM-L6-v2'...\n",
      "   - Generating new embeddings from enriched text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 17/17 [00:00<00:00, 24.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Building final FAISS index...\n",
      "\n",
      "✅✅✅ Mission Accomplished! The final, intelligent model has been built and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEANED_CSV_PATH = 'IPC_Sections_cleaned.csv'\n",
    "MODEL_SAVE_DIR = '../backend/apps/mlengine/saved_models/'\n",
    "DATA_LOOKUP_PATH = os.path.join(MODEL_SAVE_DIR, 'ipc_data_for_index.pkl')\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: ENRICH DATA WITH SPECIALIST KNOWLEDGE\n",
    "# ==============================================================================\n",
    "print(\"--- Step 1: Enriching Data ---\")\n",
    "\n",
    "df = pd.read_csv(CLEANED_CSV_PATH)\n",
    "# Filter out rare categories to ensure data consistency\n",
    "category_counts = df['mapped_category'].value_counts()\n",
    "rare_categories = category_counts[category_counts < 2].index.tolist()\n",
    "if rare_categories:\n",
    "    df = df[~df['mapped_category'].isin(rare_categories)]\n",
    "print(f\"✅ Loaded and filtered data with {len(df)} rows.\")\n",
    "\n",
    "# --- Define specialist terms to bridge the context gap ---\n",
    "theft_keywords = \"theft stole stolen snatching robbery pickpocket\"\n",
    "nuisance_keywords = \"public annoyance disturbance loud music noise party fighting argument\"\n",
    "fraud_keywords = \"fraud cheat scam online bank account money\"\n",
    "harassment_keywords = \"harassment stalking threatening messages bother safety intimidate\"\n",
    "\n",
    "# --- Create the enriched text column ---\n",
    "# Start with the original text\n",
    "df['enriched_text'] = df['cleaned_text']\n",
    "\n",
    "def add_keywords_to_category(df, category, keywords):\n",
    "    \"\"\"Finds all laws in a category and appends keywords to their text.\"\"\"\n",
    "    df.loc[df['mapped_category'] == category, 'enriched_text'] += \" \" + keywords\n",
    "    return df\n",
    "\n",
    "# --- Teach the model by enriching categories ---\n",
    "df = add_keywords_to_category(df, 'Theft', theft_keywords)\n",
    "df = add_keywords_to_category(df, 'Public Nuisance', nuisance_keywords)\n",
    "df = add_keywords_to_category(df, 'Fraud', fraud_keywords)\n",
    "df = add_keywords_to_category(df, 'Criminal Intimidation', harassment_keywords)\n",
    "\n",
    "print(\"✅ Data has been enriched with specialist keywords.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: BUILD THE FINAL SEMANTIC MODEL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Step 2: Building the Final, Enriched Model ---\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"✅ Using device: {device.upper()}\")\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"   - Loading model: '{model_name}'...\")\n",
    "semantic_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "# Generate embeddings from the NEW 'enriched_text' column\n",
    "print(\"   - Generating new embeddings from enriched text...\")\n",
    "corpus_embeddings = semantic_model.encode(df['enriched_text'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "corpus_embeddings_np = corpus_embeddings.cpu().numpy().astype('float32')\n",
    "\n",
    "# Build and save the final FAISS index\n",
    "print(\"   - Building final FAISS index...\")\n",
    "embedding_dimension = corpus_embeddings_np.shape[1]\n",
    "final_faiss_index = faiss.IndexFlatL2(embedding_dimension)\n",
    "final_faiss_index.add(corpus_embeddings_np)\n",
    "\n",
    "# Save the final assets\n",
    "faiss.write_index(final_faiss_index, os.path.join(MODEL_SAVE_DIR, 'faiss_index.index'))\n",
    "df_for_lookup = df[['section_number', 'title', 'short_description']].reset_index(drop=True)\n",
    "df_for_lookup.to_pickle(DATA_LOOKUP_PATH)\n",
    "\n",
    "print(\"\\n✅✅✅ Mission Accomplished! The final, intelligent model has been built and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b0adda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Loading and Enriching Data ---\n",
      "✅ Loaded and filtered data with 526 rows.\n",
      "✅ Data has been enriched with specialist keywords.\n",
      "\n",
      "--- Part 2: Training Final Classifier Pipelines ---\n",
      "✅ Urgency Classifier Pipeline saved.\n",
      "✅ Category Classifier Pipeline saved.\n",
      "\n",
      "--- Part 3: Building Final Recommendation Model ---\n",
      "✅ Using device: CUDA\n",
      "   - Generating new embeddings from enriched text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 17/17 [00:00<00:00, 24.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Building final FAISS index...\n",
      "\n",
      "✅✅✅ DEFINITIVE MODELS BUILT AND SAVED SUCCESSFULLY! ✅✅✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- Configuration ---\n",
    "CLEANED_CSV_PATH = 'IPC_Sections_cleaned.csv'\n",
    "MODEL_SAVE_DIR = '../backend/apps/mlengine/saved_models/'\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: LOAD AND ENRICH THE DATASET\n",
    "# ==============================================================================\n",
    "print(\"--- Part 1: Loading and Enriching Data ---\")\n",
    "df = pd.read_csv(CLEANED_CSV_PATH)\n",
    "# Filter out rare categories for stability\n",
    "category_counts = df['mapped_category'].value_counts()\n",
    "rare_categories = category_counts[category_counts < 2].index.tolist()\n",
    "if rare_categories:\n",
    "    df = df[~df['mapped_category'].isin(rare_categories)]\n",
    "print(f\"✅ Loaded and filtered data with {len(df)} rows.\")\n",
    "\n",
    "# --- Define specialist terms to bridge the context gap ---\n",
    "theft_keywords = \"theft stole stolen snatching robbery pickpocket chain\"\n",
    "nuisance_keywords = \"public annoyance disturbance loud music noise party fighting argument\"\n",
    "fraud_keywords = \"fraud cheat scam online bank account money\"\n",
    "harassment_keywords = \"harassment stalking threatening messages bother safety intimidate\"\n",
    "\n",
    "# --- Create the enriched text column for the recommendation model ---\n",
    "df['enriched_text'] = df['cleaned_text']\n",
    "def add_keywords_to_category(df, category, keywords):\n",
    "    df.loc[df['mapped_category'] == category, 'enriched_text'] += \" \" + keywords\n",
    "    return df\n",
    "\n",
    "df = add_keywords_to_category(df, 'Theft', theft_keywords)\n",
    "df = add_keywords_to_category(df, 'Public Nuisance', nuisance_keywords)\n",
    "df = add_keywords_to_category(df, 'Fraud', fraud_keywords)\n",
    "df = add_keywords_to_category(df, 'Criminal Intimidation', harassment_keywords)\n",
    "print(\"✅ Data has been enriched with specialist keywords.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: TRAIN AND SAVE BULLETPROOF CLASSIFIER PIPELINES\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Part 2: Training Final Classifier Pipelines ---\")\n",
    "X = df['cleaned_text']\n",
    "y_urgency = df['urgency_label']\n",
    "y_category = df['mapped_category']\n",
    "\n",
    "X_train, X_test, y_urgency_train, y_urgency_test, y_category_train, y_category_test = train_test_split(\n",
    "    X, y_urgency, y_category, test_size=0.2, random_state=42, stratify=y_category\n",
    ")\n",
    "\n",
    "# --- Urgency Classifier Pipeline ---\n",
    "urgency_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "urgency_pipeline.fit(X_train, y_urgency_train)\n",
    "joblib.dump(urgency_pipeline, os.path.join(MODEL_SAVE_DIR, 'urgency_classifier.joblib'))\n",
    "print(\"✅ Urgency Classifier Pipeline saved.\")\n",
    "\n",
    "# --- Category Classifier Pipeline ---\n",
    "category_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "category_pipeline.fit(X_train, y_category_train)\n",
    "joblib.dump(category_pipeline, os.path.join(MODEL_SAVE_DIR, 'category_classifier.joblib'))\n",
    "print(\"✅ Category Classifier Pipeline saved.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: BUILD THE FINAL RECOMMENDATION MODEL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Part 3: Building Final Recommendation Model ---\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"✅ Using device: {device.upper()}\")\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "semantic_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "print(\"   - Generating new embeddings from enriched text...\")\n",
    "corpus_embeddings = semantic_model.encode(df['enriched_text'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "corpus_embeddings_np = corpus_embeddings.cpu().numpy().astype('float32')\n",
    "\n",
    "print(\"   - Building final FAISS index...\")\n",
    "final_faiss_index = faiss.IndexFlatL2(corpus_embeddings_np.shape[1])\n",
    "final_faiss_index.add(corpus_embeddings_np)\n",
    "\n",
    "faiss.write_index(final_faiss_index, os.path.join(MODEL_SAVE_DIR, 'faiss_index.index'))\n",
    "df_for_lookup = df[['section_number', 'title', 'short_description']].reset_index(drop=True)\n",
    "df_for_lookup.to_pickle(DATA_LOOKUP_PATH)\n",
    "\n",
    "print(\"\\n✅✅✅ DEFINITIVE MODELS BUILT AND SAVED SUCCESSFULLY! ✅✅✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"IPC_Sections_Final.csv\")\n",
    "\n",
    "# 1. Drop rows with any empty/null values\n",
    "df = df.dropna()\n",
    "\n",
    "# 2. Drop unwanted columns\n",
    "columns_to_remove = [\"urgent_label\", \"keyword\"]\n",
    "df = df.drop(columns=columns_to_remove, errors=\"ignore\")  # ignore if not found\n",
    "\n",
    "# 3. Save the cleaned CSV\n",
    "df.to_csv(\"IPC_Sections_explore.csv\", index=False)\n",
    "\n",
    "print(\"✅ Cleaning complete! Saved as IPC_Sections_explore.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
